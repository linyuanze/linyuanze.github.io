<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>One Tourist</title>
  
  <subtitle>一名划水者的日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-28T07:34:18.139Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Lin Yuanze</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Summary for CVPR 2019!</title>
    <link href="http://yoursite.com/2018/11/28/write_semester_end/"/>
    <id>http://yoursite.com/2018/11/28/write_semester_end/</id>
    <published>2018-11-28T11:49:25.000Z</published>
    <updated>2018-11-28T07:34:18.139Z</updated>
    
    <content type="html"><![CDATA[<p>最近算是把自己的paper投稿到了CVPR 2019，在这个过程中，独立完成实验，独立撰写Paper，独立修改论文，并且每次修改后，我都会跑去学校的复印店打印修改后的paper，前前后后打印了十几份，甚至连打印店的小哥都认识我了（笑），并且修改的论文文档多达20多次，不管如何，这是一次尝试更是一次学习的机会，也确确实实实学习到了许许多多的东西。</p><p>每次看到别人能够有老师或则师兄师姐指导，很是羡慕，但是anyway，孤独不是坏事，即使一个人能力有限，但是我也会竭尽全力。接下来就要开始一年的gap，老实说，心理很恐慌，害怕担心与焦虑一直怀绕着，要好好规划好接下来的一年啊！加油加油！林远泽你可以的！</p><p>最后，希望这次的投稿能中吧！（zzzz嘻嘻）</p><div class="figure"><img src="/2018/11/28/write_semester_end/first.jpg"></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近算是把自己的paper投稿到了CVPR 2019，在这个过程中，独立完成实验，独立撰写Paper，独立修改论文，并且每次修改后，我都会跑去学校的复印店打印修改后的paper，前前后后打印了十几份，甚至连打印店的小哥都认识我了（笑），并且修改的论文文档多达20多次，不管如
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>YOLO9000:Better, Faster, Stronger</title>
    <link href="http://yoursite.com/2018/04/28/yolo9000/"/>
    <id>http://yoursite.com/2018/04/28/yolo9000/</id>
    <published>2018-04-28T15:27:42.000Z</published>
    <updated>2018-05-16T14:28:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract:</h2><p>这篇工作是在YOLO的基础上, 作者又进一步优化提出来的, 本文中提出了YOLOv2和YOLO9000, 前者在<code>VOC 2007</code>数据集上可以达到<code>67 FPS</code>, 在<code>40 FPS</code>下可以达到<code>76.8 mAP</code>, 性能已经超过了<code>Faster R-CNN</code>和<code>SSD</code>, 并提出了一种<code>联合训练分类和检测</code>(<code>joint training algorithm</code>)的方法, YOLO9000又对YOLO v2进行优化, 能够实时检测9418类物体, 故名为YOLO9000, 联合训练方法能够使得YOLO9000能够预测未被标注的检测数据目标类别的检测结果, 尽管在<code>ImageNet Detection Valset</code>上, 在200类中只有44类的检测数据, 它能够达到<code>mAP 19.7</code>的结果, 而在<code>COCO</code>数据集上没有的156类, YOLO9000能够达到<code>16.0 mAP</code>, 本篇Blog将对该篇工作进行详细介绍。</p><h2 id="introduction">Introduction:</h2><p><code>Object Detection</code>的普遍目的是为了能够<code>又快</code>、<code>又准</code>、并能<code>检测大量类别的目标</code>, 由于<code>Neural Network</code>的引入, 现在的检测框架越来越<code>快</code>、越来越<code>精确</code>, 但是<code>大多数</code>的检测方法仍然只能检测<code>少量</code>的目标。</p><p>目前的目标检测数据集大幅少于分类及标记任务的数据集的大小, 由于标注检测数据要相对更为<code>expensive</code>, 因此本文针对本问题提出了一种新的方法, <code>使用目标分类的层次视图来结合不同的数据集</code>, 以此通过已有的大量分类数据来扩大目前检测系统的检测规模, 并提出了一种的新的方法, 能够<code>在分类及检测数据集上进行联合训练</code>: 通过标记的检测图像来学习精确地定位目标, 同时使用分类图像来增加<code>vocabulary</code>和<code>robustness</code>。</p><p>于是本文主要是在YOLO上进行优化得到YOLOv2, 而通过使用上述的<code>dataset comnbination method</code>及<code>joint training algorithm</code>在<code>ImageNet</code>数据集(<code>多于9000类目标</code>)和<code>COCO</code>检测数据集上进行训练, 来获得YOLO9000。 <img src="/2018/04/28/yolo9000/1.jpg" alt="YOLO9000!"></p><h2 id="better">Better:</h2><p>实际上, 与目前最先进的检测算法相对, YOLO存在还许多缺陷, 例如YOLO与Faster R-CNN相对, 容易产生大量的定位错误, 并且与<code>Region Proposal</code>方法相比, YOLO的召回率较低, 因此接下来的工作主要是在保证分类精度的基础上, 针对<code>Recall</code>及<code>localization</code>来进行优化的, 并且与大部分方法(<code>增大模型</code>或<code>融合多个模型</code>)来提升性能不同, YOLOv2对网络进行了简化, 使得更容易学习到<code>representation</code>的能力。</p><h3 id="batch-normalization">Batch Normalization:</h3><p><code>Batch Normalization</code>能够巨大地改善收敛情况并能减少其他形式的正则化的需求, 通过在YOLO中的<code>所有卷积层</code>里加入<code>Batch Normalization</code>, 作者在YOLO上<code>mAP</code>提高了<code>2%</code>, 感觉很interesting, 并且<code>Batch Normalization</code>也帮助正则化模型, 加入<code>Batch Normalization</code>后, 可以去掉<code>Dropout</code>, 并且没有出现<code>overfitting</code>。</p><h3 id="high-resolution-classifier">High Resolution Classifier:</h3><p>目前所有<code>stage-of-the-art</code>的检测算法都使用在<code>ImageNet</code>数据集上<code>pre-trained</code>的模型, 从<code>AlexNet</code>开始, 许多的分类模型都在小于<code>256x256</code>分辨率上的输入图像上进行操作, 原始的YOLO在<code>224x224</code>图片上训练分类网络, 并增加分辨率至<code>448x448</code>来训练'detector', 这意味着网络需要<code>同时学习目标检测能力</code>并且<code>还要调整新的输入分辨率</code>。</p><p>对于YOLOv2, 作者首先使用分类网络在<code>ImageNet</code>上<code>full 448x448的分辨率</code>进行<code>10个epoch</code>的<code>fine tune</code>, 这能够使得<code>filters</code>在<code>higher resolution input</code>上更好地工作, 之后再将得到的网络在检测数据集上进行<code>fine tune</code>, <code>high resolution classification</code>网络差不多能够提升<code>4% mAP</code>的性能, what???(黑人问号, 提这么多。。Unbelivable)</p><h3 id="convolutional-with-anchor-boxes">Convolutional With Anchor Boxes:</h3><p>YOLO直接在<code>convolutional feature extractor</code>上使用全连接层, 来直接预测<code>Bounding Boxes</code>的坐标, 而Faster R-CNN则是使用<code>hand-picked priors</code>来预测<code>Bounding boxes</code>, 即Faster-RCNN中的<code>RPN</code>仅使用<code>卷积层</code>来预测<code>anchor boxes</code>的<code>offsets</code>及<code>confidences</code>, 由于<code>prediction layer</code>是<code>convolutional</code>, <code>RPN</code>在<code>feature map</code>上的每个位置都预测这些<code>offsets</code>, 预测<code>offsets</code>而不是<code>coordinates</code>能够简化检测问题, 并且使得网络更多地学习<code>(~~作者是这么说, emmm, I need some time to think)</code>。</p><p>于是作者将YOLO中的<code>全连接层</code>去掉(原先是用来做<code>prediction</code>的), 使用<code>anchor boxes</code>来预测<code>bouding boxes</code>, 并且呢, 作者还移去了一个<code>pooling layer</code>, 为了使得网络上的<code>convolutional layers</code>的输出有更大的分辨率(~~心机boy), 并收缩网络到<code>416x416</code>的输入图像上(而不是之前提到的<code>448x448</code>), 作者这么做是为了得到的<code>feature map</code>上的<code>locations</code>是<code>奇数的</code>, this matters, 从而得到<code>a single center cell</code>, <code>large objects</code>更倾向于占据图像的<code>center</code>, 因此在<code>center</code>位置上, 使用<code>a single location</code>来预测目标更好, 而不是<code>4个</code>位置都在附近, YOLO的<code>卷积层</code>下采样输入图像<code>32倍</code>, 由于使用<code>416x416</code>的输入图像, 最终得到的输出特征图是<code>13x13</code>。</p><p>当使用<code>anchor boxes</code>后, 从<code>spatial location</code>中解耦<code>class prediciton mechanism</code>, 并对每一个<code>anchor box</code>, 都预测<code>class and objectness</code>, 与YOLO一样, <code>objectness prediction</code>也预测<code>ground truth</code>与<code>proposed box</code>间的<code>IOU</code>, <code>class predictions</code>预测含有目标的location属于某类的条件概率, 使用<code>anchor boxes</code>后, 性能有所降低, 原来YOLO每张图像只预测<code>98个boxes</code>, 现在通过<code>anchor boxes mechanism</code>能预测<code>多于1000的boxes</code>, 在没有<code>anchor boxes</code>下, 中间模型性能为<code>69.5 mAP和81% Recall</code>, 使用<code>anchor boxes</code>后, 模型性能为<code>69.2 mAP及88% Recall</code>, 尽管<code>mAP</code>下降了, 但是<code>Recall</code>得到了<code>7%</code>的提升。</p><h3 id="dimension-clusters">Dimension Clusters:</h3><p><code>Better box dimensions priors</code>能够帮助模型更容易地预测好的检测结果, 作者不是通过hand-picked来选择<code>box dimensions</code>, 而是在<code>training set bouding boxes</code>上运行<code>k-means clustering</code>来自动地找到好的<code>prior</code>, 若使用标准的<code>k-means</code>算法(<code>欧式距离</code>), 大的boxes比起小的boxes产生更大的错误, 为了使得有更好多的<code>IOU scores</code>, 作者选用的距离标准是: <img src="/2018/04/28/yolo9000/3.jpg" alt="Distance Metric!"></p><p>作者经过实验得出, <code>k=5</code>时,在模型复杂性和高召回率之间有一个很好的权衡, 得到的<code>cluster centroids</code>很少是<code>短的</code>, <code>宽的</code>boxes, 而更多是<code>高的</code>, <code>瘦的</code>的boxes。 <img src="/2018/04/28/yolo9000/2.jpg" alt="K-means Clustering!"></p><p>并将作者设计的<code>distance下的k-means</code>聚类与<code>hand-picked anchor boxes</code>进行比较, 充分表明<code>k-means</code>聚类能够更好地帮助模型产生更好表现力的<code>bounding box</code>, 结果如下图所示: <img src="/2018/04/28/yolo9000/4.jpg" alt="IOU Results!"></p><h3 id="direct-location-prediction">Direct Location Prediction:</h3><h3 id="fine-grained-features">Fine-Grained Features:</h3><h3 id="multi-scale-training">Multi-Scale Training:</h3><h3 id="further-experiments">Further Experiments</h3><h2 id="faster">Faster:</h2><h3 id="darknet-19">Darknet-19</h3><h3 id="training-for-classification">Training For Classification</h3><h3 id="training-for-detection">Training For Detection</h3><h2 id="stronger">Stronger:</h2><h3 id="hierarchical-classification">Hierarchical Classification:</h3><h3 id="dataset-combination-with-wordtree">Dataset Combination With WordTree:</h3><h3 id="joint-classification-and-detection">Joint Classification And Detection</h3><h2 id="summary">Summary</h2><h2 id="reference">Reference</h2><p>YOLO9000: Better, Faster, Stronger. Joseph Redmon, Ali Farhadi. CVPR2017</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;
&lt;p&gt;这篇工作是在YOLO的基础上, 作者又进一步优化提出来的, 本文中提出了YOLOv2和YOLO9000, 前者在&lt;code&gt;VOC 2007&lt;/code&gt;数据集上可以达到&lt;code&gt;67 FPS&lt;/code&gt;
      
    
    </summary>
    
      <category term="Computer Vision, Deep Learning, Object Detection" scheme="http://yoursite.com/categories/Computer-Vision-Deep-Learning-Object-Detection/"/>
    
    
  </entry>
  
  <entry>
    <title>Go And Try !!!</title>
    <link href="http://yoursite.com/2018/04/28/hello-world/"/>
    <id>http://yoursite.com/2018/04/28/hello-world/</id>
    <published>2018-04-28T11:49:25.000Z</published>
    <updated>2018-04-28T12:04:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天开始写博客啦！会定期更新一些看过paper的读后感，记录自己学习的点点滴滴，加油! Everything is possible, no matter how difficult it is, I will try for my dream!!!</p><div class="figure"><img src="/2018/04/28/hello-world/first.jpg" alt="Go and Try!"><p class="caption">Go and Try!</p></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天开始写博客啦！会定期更新一些看过paper的读后感，记录自己学习的点点滴滴，加油! Everything is possible, no matter how difficult it is, I will try for my dream!!!&lt;/p&gt;
&lt;div cl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>You Only Look Once:Unified, Real-Time Object Detection</title>
    <link href="http://yoursite.com/2018/04/28/YOLO/"/>
    <id>http://yoursite.com/2018/04/28/YOLO/</id>
    <published>2018-04-28T04:44:10.000Z</published>
    <updated>2018-05-16T07:06:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction">Introduction:</h2><p>这是一篇很经典的关于one-stage的paper, 最近由于组里要做一些检测的工作，并且是想让检测算法能够在嵌入式平台上运行, 因此主要聚焦在<code>one-stage detector</code>上, 而两篇比较经典的<code>one-stage detector</code>的paper就是<code>SSD</code>和<code>YOLO</code>了。</p><p>本篇工作相对于Faster-RCNN, 不需要先训RPN再去训Fast RCNN, 即不需要额外去训练网络来预测潜在的<code>proposal</code>, 可以实现<code>end to end</code>的训练, 虽然<code>mAP</code>上没有达到Faster RCNN那种程度，但速度有了大幅度的提升，使用GPU甚至可以达到实时地检测，YOLO将目标检测的问题看成一个回归问题来进行求解, 完成了从输入的原始图像到物体位置和类别的输出, 整个YOLO系统Pipeline如下： <img src="/2018/04/28/YOLO/1.jpg" alt="YOLO System Pipeline!"></p><p>即:</p><ol style="list-style-type: decimal"><li><code>Resize</code> input image to <code>448 x 448</code></li><li>Runs <code>a single CNN</code> on the image</li><li><code>NMS</code>(thresholds the detection results by their corresponding confidence)</li></ol><p>YOLO的检测网络整体包括<code>24个卷积层</code>和<code>2个全连接层</code>, 网络组成结果如下图所示(并使用<code>1x1卷积层</code>来减少来自前面层的特征空间，来达到减少FLOPS来增快推理速度的目的): <img src="/2018/04/28/YOLO/3.jpg" alt="YOLO System Pipeline!"></p><p>其中通过卷积层来对图像进行特征提取的操作，后续的全连接层则是用于预测图像中目标的位置及其类别概率值, 并在<code>ImageNet</code>数据集(<code>224x224</code>分辨率)上预训练好卷积层部分权重, 之后将分辨率扩大为<code>448x448</code>(即翻倍)来训练最终的整个检测部分。</p><h2 id="more-details">More Details：</h2><p>YOLO将输入图像分成<code>SxS</code>个格子, 如下图所示 <img src="/2018/04/28/YOLO/2.jpg" alt="Input Image!"></p><p>每个格子负责检测‘落入’该格子的物体。若某个物体的中心位置的坐标落入到某个格子，那么这个格子就负责检测出这个物体。如下图所示，图中物体狗的中心点（红色原点）落入第<code>5</code>行、第<code>2</code>列的格子内，所以这个格子负责预测图像中的物体狗。 <img src="/2018/04/28/YOLO/5.jpg" alt="Image"></p><p>每个格子输出<code>B</code>个<code>bounding box</code>（包含物体的矩形区域）信息，以及<code>C</code>个物体属于某种类别的概率信息, <code>Bounding box</code>信息包含<code>5</code>个数据值，分别是<code>x,y,w,h,confidence</code>。其中<code>x,y</code>是指当前格子预测得到的物体的<code>bounding box</code>的中心位置的坐标。<code>w,h</code>是<code>bounding box</code>的宽度和高度。注意：实际训练过程中，<code>w</code>和<code>h</code>的值使用图像的宽度和高度进行归一化到<code>[0,1]</code>区间内；<code>x，y</code>是<code>bounding box</code>中心位置相对于当前格子位置的偏移值，并且被归一化到<code>[0,1]</code>。</p><p><code>confidence</code>反映当前<code>bounding box</code>是否包含物体以及物体位置的准确性，计算方式如下：</p><p><code>confidence = P(object) * IOU</code>, 其中，若bounding box包含物体，则<code>P(object) = 1</code>；否则<code>P(object) = 0</code>, <code>IOU(intersection over union)</code>为预测bounding box与物体真实区域的交集面积（以像素为单位，用真实区域的像素面积归一化到[0,1]区间)。</p><p>因此，YOLO网络最终的全连接层的输出维度是<code>S * S * (B * 5 + C)</code>。YOLO论文中，作者训练采用的输入图像分辨率是<code>448x448</code>，<code>S=7，B=2</code>；采用<code>VOC</code> 2类标注物体作为训练数据，<code>C=20</code>。因此输出向量为<code>7 * 7 * (20 + 2 * 5)=1470维</code>。作者开源出的YOLO代码中，全连接层输出特征向量各维度对应内容如下： <img src="/2018/04/28/YOLO/1.gif" alt="Image2"></p><p>每一个栅格还要预测<code>C</code>个<code>conditional class probability(条件类别概率)</code>: <code>Pr(Classi|Object)</code>。即在一个栅格包含一个Object的前提下，它属于某个类的概率, 我们只为每个栅格预测一组（<code>C个</code>）类概率，而不考虑框B的数量, 在测试阶段，将每个栅格的<code>conditional class probabilities</code>与每个<code>bounding box</code>的<code>confidence</code>相乘: <img src="/2018/04/28/YOLO/0.jpg" alt="Image0"></p><p>这样既可得到每个<code>bounding box</code>的具体类别的<code>confidence score</code>, 这乘积既包含了<code>bounding box</code>中预测的class的 probability信息，也反映了<code>bounding box</code>是否含有Object和bounding box坐标的准确度。 <img src="/2018/04/28/YOLO/10_1.gif" alt="Image0"></p><p>当在<code>PASCAL VOC</code>数据集上使用YOLO时，论文中指定<code>S=7</code>, 将每一张图片分成<code>7x7</code>的网格, 并且每个网格预测<code>B=2</code>个<code>Boxes</code>, 并且<code>每个Box</code>上有<code>x,y,w,h,confidence</code>等<code>5</code>个预测值, 同时<code>C=20</code>(即刚好是PASCAL VOC上的类别数), 因此最终会有<code>98</code>个<code>Bounding Boxes</code>, 最终预测的是<code>7 * 7 * 30</code>即<code>(S * S (B * 5 + C))</code>的Tensor。 <img src="/2018/04/28/YOLO/12_1.gif" alt="Image0"></p><h2 id="loss-function">Loss Function：</h2><p>YOLO使用均方和误差作为loss函数来优化模型参数，即网络输出的<code>S * S * (B * 5 + C)</code>维向量与真实图像的对应<code>S * S * (B * 5 + C)</code>维向量的均方和误差。如下式所示。其中，<code>coordError、iouError和classError</code>分别代表<code>预测数据与标定数据之间的坐标误差</code>、<code>IOU误差</code>和<code>分类误差</code>。 <img src="/2018/04/28/YOLO/7.jpg" alt="Image3"></p><p>YOLO对上式loss的计算进行了如下修正。</p><p>[1] 位置相关误差(坐标、<code>IOU</code>)与分类误差对网络<code>loss</code>的贡献值是不同的，因此YOLO在计算loss时，使用<span class="math inline">\(\lambda _{coord} =5\)</span>修正<code>coordError</code>。</p><p>[2] 在计算<code>IOU</code>误差时，包含物体的格子与不包含物体的格子，二者的<code>IOU</code>误差对网络<code>loss</code>的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的<code>confidence</code>值近似为0，变相放大了包含物体的格子的<code>confidence</code>误差在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用<span class="math inline">\(\lambda_{noobj}\)</span> = 0.5修正<code>iouError</code>。（注此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内）。</p><p>[3] 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（<code>w和h</code>）进行求平方根来改进这个问题。（注：这个方法并不能完全解决这个问题）。</p><p>综上，YOLO在训练过程中Loss计算如下式所示： <img src="/2018/04/28/YOLO/8.jpg" alt="Image4"> 其中，<code>x,y,w,C,p</code>为网络预测值，<code>x,y,w,C</code>和<span class="math inline">\(\widehat{p}\)</span>为标注值。<span class="math inline">\(\Pi_{i}^{obj}\)</span> 表示物体落入格子i中，<span class="math inline">\(\Pi_{ij}^{obj}\)</span>和<span class="math inline">\(\Pi_{ij}^{noobj}\)</span>分别表示物体落入与未落入格子<code>i</code>的第<code>j</code>个<code>bounding box</code>内。</p><h3 id="note">Note:</h3><pre><code>1.YOLO方法模型训练依赖于物体识别标注数据，因此，对于非常规的物体形状或比例，YOLO的检测效果并不理想。2.YOLO采用了多个下采样层，网络学到的物体特征并不精细，因此也会影响检测效果。3.YOLO loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。 </code></pre><h2 id="training-details">Training Details:</h2><p>YOLO模型训练分为两步：</p><p>1）预训练。使用<code>ImageNet 1000类</code>数据训练YOLO网络的<code>前20个卷积层</code> + <code>1个average池化层</code> + <code>1个全连接层</code>。训练图像分辨率<code>resize</code>到<code>224x224</code>。</p><p>2）用步骤1）得到的<code>前20个卷积层</code>网络参数来初始化YOLO模型前<code>20</code>个卷积层的网络参数，然后用VOC <code>20</code>类标注数据进行YOLO模型训练。为提高图像精度，在训练检测模型时，将输入图像分辨率<code>resize</code>到<code>448x448</code>。</p><p>在 YOLO中，每个栅格预测多个<code>bounding box</code>，但在网络模型的训练中，希望每一个物体最后由一个<code>bounding box predictor</code>来负责预测, 因此，当前哪一个<code>predictor</code>预测的<code>bounding box</code>与<code>ground truth box</code>的<code>IOU</code>最大，这个<code>predictor</code>就负责<code>predict object</code>, 这会使得每个predictor可以专门的负责特定的物体检测。随着训练的进行，每一个<code>predictor</code>对特定的物体尺寸、长宽比的物体的类别的预测会越来越好。</p><h3 id="advantages">Advantages：</h3><ol style="list-style-type: decimal"><li><p><em>快</em>。YOLO将物体检测作为回归问题进行求解，整个检测网络pipeline简单, 在<code>titan X GPU</code>上，在保证检测准确率的前提下<code>(63.4% mAP，VOC 2007 test set)</code>，可以达到<code>45fps</code>的检测速度。</p></li><li><p><em>背景误检率低</em>。YOLO在训练和推理过程中能‘看到’整张图像的整体信息，而基于<code>region proposal</code>的物体检测方法（如<code>rcnn/fast rcnn</code>），在检测过程中，只‘看到’候选框内的局部图像信息。因此，若当图像背景（非物体）中的部分数据被包含在候选框中送入检测网络进行检测时，容易被误检测成物体。测试证明，YOLO对于背景图像的误检率低于fast rcnn误检率的一半, 论文给出对比结果如下: <img src="/2018/04/28/YOLO/16.jpg" alt="Image26"></p></li><li><p><em>通用性强</em>。YOLO对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于<code>DPM</code>和<code>RCNN</code>系列检测方法。</p></li></ol><p>但相比RCNN系列物体检测方法，YOLO具有以下缺点：</p><p>1)<em>识别物体位置精准性差</em>。 2)<em>召回率低</em>。</p><h2 id="inference-details">Inference Details:</h2><div class="figure"><img src="/2018/04/28/YOLO/3_1.gif" alt="Detection Procedure"><p class="caption">Detection Procedure</p></div><h3 id="nms">NMS:</h3><div class="figure"><img src="/2018/04/28/YOLO/4_1.gif" alt="Non-Maximum Suppression"><p class="caption">Non-Maximum Suppression</p></div><h3 id="detection-results">Detection Results:</h3><div class="figure"><img src="/2018/04/28/YOLO/5_1.gif" alt="Inference Pipeline"><p class="caption">Inference Pipeline</p></div><h2 id="experimental-results">Experimental Results:</h2><p>将YOLO与其他检测算法进行对比: <img src="/2018/04/28/YOLO/15.jpg" alt="Image6"></p><div class="figure"><img src="/2018/04/28/YOLO/17.jpg" alt="Experimental Results"><p class="caption">Experimental Results</p></div><h2 id="generation-results-on-datasets">Generation Results On Datasets:</h2><div class="figure"><img src="/2018/04/28/YOLO/20.jpg" alt="Generation Results"><p class="caption">Generation Results</p></div><h2 id="summary">Summary:</h2><p>YOLO与<code>Faster-RCNN</code>的检测pipeline不同，开辟了<code>one stage detector</code>的思路，大幅度地提高了检测的速度，虽然比起<code>Faster-RCNN</code>精度有所下降，但是给我们带来了实时检测的可能，正是这种创新性的工作像灯塔一般引领着我们，感谢每位Researcher的工作，向你们致敬！</p><h2 id="reference">Reference:</h2><p>You Only Look Once:Unified, Real-Time Object Detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. CVPR 2016</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;introduction&quot;&gt;Introduction:&lt;/h2&gt;
&lt;p&gt;这是一篇很经典的关于one-stage的paper, 最近由于组里要做一些检测的工作，并且是想让检测算法能够在嵌入式平台上运行, 因此主要聚焦在&lt;code&gt;one-stage detector
      
    
    </summary>
    
      <category term="Computer Vision, Deep Learning, Object Detection" scheme="http://yoursite.com/categories/Computer-Vision-Deep-Learning-Object-Detection/"/>
    
    
  </entry>
  
</feed>
